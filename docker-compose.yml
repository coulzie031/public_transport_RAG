services:
  # --- SCHEDULER: Triggers Python scripts based on time ---
  scheduler:
    image: mcuadros/ofelia:latest
    container_name: scheduler
    depends_on:
      - kafka
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: daemon --docker
    restart: unless-stopped

  # --- INFRASTRUCTURE: KAFKA (KRaft Mode) ---
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms512M" 
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 1024M

  # --- WORKER: STATION PRODUCER ---
  station-producer:
    build: .
    container_name: station-producer
    restart: unless-stopped
    depends_on:
      - kafka
    volumes:
      - ./scripts:/app/scripts
    environment:
      KAFKA_SERVER: "kafka:29092"
      PRIM_TOKEN: ${PRIM_TOKEN}
      PYTHONUNBUFFERED: "1"
    labels:
      ofelia.enabled: "true"
      ofelia.job-exec.station-sync.schedule: "@every 6s"
      # FIX: Ensure this filename matches your scripts/ folder!
      ofelia.job-exec.station-sync.command: "python /app/scripts/station_producer_2.py"
    command: tail -f /dev/null
    deploy:
      resources:
        limits:
          cpus: '0.10'
          memory: 128M

  # --- WORKER: ALERT PRODUCER ---
  alert-producer:
    build: .
    container_name: alert-producer
    restart: unless-stopped
    depends_on:
      - kafka
    volumes:
      - ./scripts:/app/scripts
    environment:
      KAFKA_SERVER: "kafka:29092"
      PRIM_TOKEN: ${PRIM_TOKEN}
      PYTHONUNBUFFERED: "1"
    labels:
      ofelia.enabled: "true"
      ofelia.job-exec.alert-sync.schedule: "@every 5m"
      ofelia.job-exec.alert-sync.command: "python /app/scripts/producer_alerts.py"
    command: tail -f /dev/null
    deploy:
      resources:
        limits:
          cpus: '0.10'
          memory: 128M

  # --- SEARCH: ELASTICSEARCH ---
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    container_name: elasticsearch
    restart: unless-stopped
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" 
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 1024M

  # --- SINK: MOVING DATA FROM KAFKA TO ELASTIC ---
  app:
    build: . 
    container_name: app-sink
    depends_on:
      - kafka
      - elasticsearch
    volumes:
      - ./scripts:/app/scripts
    environment:
      KAFKA_SERVER: "kafka:29092"
      ELASTIC_SERVER: "http://elasticsearch:9200"
      PYTHONUNBUFFERED: "1"
    # The Sink should run continuously, not on a schedule
    command: python /app/scripts/stations_sink.py
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
